# LM curve fitting
# curve_fit is borrowed from https://github.com/huangli712/ACFlow
"""
    OnceDifferentiable

Mutable struct. It is used for objectives and solvers where the gradient
is available/exists.

### Members
* â„±! -> Objective. It is actually a function call and return objective.
* ğ’¥! -> It is a function call as well and returns jacobian of objective.
* ğ¹  -> Cache for â„±! output.
* ğ½  -> Cache for ğ’¥! output.
"""
mutable struct OnceDifferentiable
    â„±!::Any
    ğ’¥!::Any
    ğ¹::Any
    ğ½::Any
end

"""
    OnceDifferentiable(ğ‘“, p0::AbstractArray, ğ¹::AbstractArray)

Constructor for OnceDifferentiable struct. `ğ‘“` is the function, `p0` is
the inital guess, `ğ¹ = ğ‘“(p0)` is the cache for `ğ‘“`'s output.
"""
function OnceDifferentiable(ğ‘“, p0::AbstractArray, ğ¹::AbstractArray)
    # Backup ğ‘“(x) to ğ¹.
    function â„±!(ğ¹, x)
        return copyto!(ğ¹, ğ‘“(x))
    end

    # Calculate jacobian for ğ‘“(x), the results are stored in ğ½.
    # The finite difference method is used.
    function ğ’¥!(ğ½, x)
        rel_step = cbrt(eps(real(eltype(x))))
        abs_step = rel_step
        @inbounds for i in 1:length(x)
            xâ‚› = x[i]
            Ïµ = max(rel_step * abs(xâ‚›), abs_step)
            x[i] = xâ‚› + Ïµ
            fâ‚‚ = vec(ğ‘“(x))
            x[i] = xâ‚› - Ïµ
            fâ‚ = vec(ğ‘“(x))
            ğ½[:, i] = (fâ‚‚ - fâ‚) ./ (2 * Ïµ)
            x[i] = xâ‚›
        end
    end

    # Create memory space for jacobian matrix
    ğ½ = eltype(p0)(NaN) .* vec(ğ¹) .* vec(p0)'

    # Call the default constructor
    return OnceDifferentiable(â„±!, ğ’¥!, ğ¹, ğ½)
end

"""
    value(obj::OnceDifferentiable)

Return `obj.ğ¹`. `obj` will not be affected.
"""
value(obj::OnceDifferentiable) = obj.ğ¹

"""
    value(obj::OnceDifferentiable, ğ¹, x)

Return `ğ‘“(x)`. `obj` will not be affected, but `ğ¹` is updated.
"""
value(obj::OnceDifferentiable, ğ¹, x) = obj.â„±!(ğ¹, x)

"""
    value!(obj::OnceDifferentiable, x)

Return `ğ‘“(x)`. `obj.ğ¹` will be updated and returned.
"""
function value!(obj::OnceDifferentiable, x)
    obj.â„±!(obj.ğ¹, x)
    return obj.ğ¹
end

"""
    jacobian(obj::OnceDifferentiable)

Return `obj.ğ½`. `obj` will not be affected.
"""
jacobian(obj::OnceDifferentiable) = obj.ğ½

"""
    jacobian(obj::OnceDifferentiable, ğ½, x)

Return jacobian. `obj` will not be affected, but `ğ½` is updated.
"""

jacobian(obj::OnceDifferentiable, ğ½, x) = obj.ğ’¥!(ğ½, x)

"""
    jacobian!(obj::OnceDifferentiable, x)

Return jacobian. `obj.ğ½` will be updated and returned.
"""
function jacobian!(obj::OnceDifferentiable, x)
    obj.ğ’¥!(obj.ğ½, x)
    return obj.ğ½
end

"""
    LMOptimizationResults{T,N}

It is used to save the optimization results of the `levenberg_marquardt`
algorithm.

### Members
* xâ‚€         -> Initial guess for the solution.
* minimizer  -> Final results for the solution.
* minimum    -> Residual.
* iterations -> Number of iterations.
* xconv      -> If the convergence criterion 1 is satisfied.
* gconv      -> If the convergence criterion 2 is satisfied.
"""
struct LMOptimizationResults{T,N}
    xâ‚€::Array{T,N}
    minimizer::Array{T,N}
    minimum::T
    iterations::Int
    xconv::Bool
    gconv::Bool
end

"""
    levenberg_marquardt(df::OnceDifferentiable, xâ‚€::AbstractVector{T})

Returns the argmin over x of `sum(f(x).^2)` using the Levenberg-Marquardt
algorithm. The function `f` is encoded in `df`. `xâ‚€` is an initial guess
for the solution.

See also: [`OnceDifferentiable`](@ref).
"""
function levenberg_marquardt(df::OnceDifferentiable, xâ‚€::AbstractVector{T} where {T})
    # Some predefined constants
    min_diagonal = 1e-6 # lower bound on values of diagonal matrix
    #
    x_tol = 1e-08 # search tolerance in x
    g_tol = 1e-12 # search tolerance in gradient
    maxIter = 1000  # maximum number of iterations
    #
    Î›â‚˜ = 1e+16 # minimum trust region radius
    Î»â‚˜ = 1e-16 # maximum trust region radius
    Î» = eltype(xâ‚€)(10) # (inverse of) initial trust region radius
    Î»áµ¢ = 10.0  # Î» is multiplied by this factor after step below min quality
    Î»áµ£ = 0.10  # Î» is multiplied by this factor after good quality steps
    #
    min_step_quality = 1e-3 # for steps below this quality, the trust region is shrinked
    good_step_quality = 0.75 # for steps above this quality, the trust region is expanded

    # First evaluation
    # Both df.ğ¹ and df.ğ½ are updated.
    # And ğ¹ and ğ½ become aliases of df.ğ¹ and df.ğ½, respectively.
    value!(df, xâ‚€)
    jacobian!(df, xâ‚€)
    ğ¹ = value(df)
    ğ½ = jacobian(df)

    # Setup convergence criteria
    converged = false
    xconv = false
    gconv = false
    iter = 0

    # Calculate ğ‘“(xâ‚€) and initial residual
    x = copy(xâ‚€)
    trial_f = similar(ğ¹)
    C_resid = sum(abs2, ğ¹)

    # Create buffers
    ğ½áµ€ğ½ = diagm(x)
    ğ½Î´x = similar(ğ¹)

    # Main iteration
    while (~converged && iter < maxIter)
        # Update jacobian ğ½ for new x
        jacobian!(df, x)

        # Solve the equation: [ğ½áµ€ğ½ + Î» diag(ğ½áµ€ğ½)] Î´ = ğ½áµ€ğ¹
        # What we want to get is Î´.
        mul!(ğ½áµ€ğ½, ğ½', ğ½)
        #
        ğ·áµ€ğ· = diag(ğ½áµ€ğ½)
        replace!(x -> x â‰¤ min_diagonal ? min_diagonal : x, ğ·áµ€ğ·)
        #
        @simd for i in eachindex(ğ·áµ€ğ·)
            @inbounds ğ½áµ€ğ½[i, i] += Î» * ğ·áµ€ğ·[i]
        end
        #
        Î´x = - ğ½áµ€ğ½ \ (ğ½' * ğ¹)

        # If the linear assumption is valid, the new residual is predicted.
        mul!(ğ½Î´x, ğ½, Î´x)
        ğ½Î´x .= ğ½Î´x .+ ğ¹
        P_resid = sum(abs2, ğ½Î´x)

        # Try to calculate new x, and then ğ¹ â‰¡ ğ‘“(x), and then the residual.
        xnew = x + Î´x
        value(df, trial_f, xnew)
        T_resid = sum(abs2, trial_f)

        # Step quality = residual change / predicted residual change
        Ï = (T_resid - C_resid) / (P_resid - C_resid)
        if Ï > min_step_quality
            # Update x, ğ‘“(x), and residual.
            x .= xnew
            value!(df, x)
            C_resid = T_resid

            # Increase trust region radius
            if Ï > good_step_quality
                Î» = max(Î»áµ£ * Î», Î»â‚˜)
            end
        else
            # Decrease trust region radius
            Î» = min(Î»áµ¢ * Î», Î›â‚˜)
        end

        # Increase the iteration
        iter += 1

        # Check convergence criteria:
        # 1. Small gradient: norm(ğ½áµ€ * ğ¹, Inf) < g_tol
        if norm(ğ½' * ğ¹, Inf) < g_tol
            gconv = true
        end
        # 2. Small step size: norm(Î´x) < x_tol
        if norm(Î´x) < x_tol * (x_tol + norm(x))
            xconv = true
        end
        # 3. Calculate converged
        converged = gconv | xconv
    end

    # Return the results
    return LMOptimizationResults(xâ‚€,      # xâ‚€
                                 x,       # minimizer
                                 C_resid, # residual
                                 iter,    # iterations
                                 xconv,   # xconv
                                 gconv)
end

"""
    LsqFitResult

It encapsulates the results for curve fitting.

### Members
* param     -> Fitted results, i.e, the fitting parameters.
* resid     -> Residuals.
* jacobian  -> Jacobian matrix.
* converged -> If the curve-fitting algorithm is converged.
"""
struct LsqFitResult
    param::Any
    resid::Any
    jacobian::Any
    converged::Any
end

"""
    curve_fit(model, x, y, p0)

Fit data to a non-linear `model`. `p0` is an initial model parameter guess.
The return object is a composite type (`LsqFitResult`).

See also: [`LsqFitResult`](@ref).
"""
function curve_fit(model, x::AbstractArray, y::AbstractArray, p0::AbstractArray)
    f = (p) -> model(x, p) - y
    r = f(p0)
    R = OnceDifferentiable(f, p0, r)
    OR = levenberg_marquardt(R, p0)
    p = OR.minimizer
    conv = OR.xconv || OR.gconv
    return LsqFitResult(p, value!(R, p), jacobian!(R, p), conv)
end

# === sigmoid loss function ===

# partial differentiation of sigmoid loss function
function âˆ‚Â²lossÏ•Divâˆ‚pÂ²(p::Vector{T}, x::Vector{T}, y::Vector{T}) where {T<:Real}
    a, b, c, d = p
    L = length(x)
    s = 1 ./ (1 .+ exp.(-d * (x .- c)))
    s1 = s .* (1 .- s)  # s1 = s * (1 - s)
    r = a .+ b * s .- y  # æ®‹å·®é¡¹

    Jaa = 2 * L
    Jbb = 2 * sum(s .^ 2)
    Jcc = 2 * b^2 * d^2 * sum(s .^ 2 .* (1 .- s) .^ 2) +
          2 * b * d^2 * sum(s1 .* (1 .- 2 * s) .* r)
    Jdd = 2 * sum(b^2 * s .^ 2 .* (1 .- s) .^ 2 .* (x .- c) .^ 2 +
                  b * (x .- c) .^ 2 .* s1 .* (1 .- 2 * s) .* r)

    Jab = 2 * sum(s)
    Jac = -2 * b * d * sum(s1)
    Jad = 2 * b * sum(s1 .* (x .- c))
    Jbc = -2 * d * sum(s1 .* (b * s .+ r))
    Jbd = 2 * sum(s1 .* (x .- c) .* (b * s .+ r))
    Jcd = -2 *
          b *
          sum(s1 .* (b * d * s1 .* (x .- c) .+ (1 .+ d * (x .- c) .* (1 .- 2 * s)) .* r))

    return [Jaa Jab Jac Jad; Jab Jbb Jbc Jbd; Jac Jbc Jcc Jcd; Jad Jbd Jcd Jdd]
end

function âˆ‚Â²lossÏ•Divâˆ‚pâˆ‚y(p::Vector{T}, x::Vector{T}, y::Vector{T}) where {T<:Real}
    a, b, c, d = p
    L = length(x)

    s = 1 ./ (1 .+ exp.(-d * (x .- c)))
    s1 = s .* (1 .- s)  # s1 = s * (1 - s)

    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix = zeros(T, 4, L)

    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[1, :] .= -2  # âˆ‚Â²loss/âˆ‚aâˆ‚y_i
    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[2, :] = -2 * s  # âˆ‚Â²loss/âˆ‚bâˆ‚y_i
    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[3, :] = 2 * b * d * s1  # âˆ‚Â²loss/âˆ‚câˆ‚y_i
    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[4, :] = -2 * b * s1 .* (x .- c)  # âˆ‚Â²loss/âˆ‚dâˆ‚y_i

    return âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix
end
#=
function âˆ‚lossÏ•Divâˆ‚y(p, x, y)
    a, b, c, d = p
    s = 1 ./ (1 .+ exp.(-d * (x .- c)))
    return -2 * (a .+ b * s .- y)
end

function lossÏ•(p, x, y)
    a, b, c, d=p
    s=1 ./ (1 .+ exp.(-d*(x .- c)))
    r=a .+ b*s - y
    return sum(r .^ 2)
end

function âˆ‚lossÏ•Divâˆ‚p(p, x, y)
    a, b, c, d=p
    s=1 ./ (1 .+ exp.(-d*(x .- c)))
    r=a .+ b*s - y
    Ja=2*sum(r)
    Jb=2*sum(s .* r)
    Jc=-2*b*d*sum(s .* (1 .- s) .* r)
    Jd=2*b*sum(s .* (1 .- s) .* (x .- c) .* r)
    return [Ja, Jb, Jc, Jd]
end
=#
